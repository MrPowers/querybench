{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"QueryBench","text":"<p>QueryBench provides high-quality benchmarks for popular engines, so you can easily identify the best tool for your workloads.</p> <p>Here are the QueryBench single table results on the 1e8 table:</p> <p></p>"},{"location":"#h2o-benchmark-results","title":"h2o benchmark results","text":"<p>This section shows the h2o benchmarks with the QueryBench execution methodology.</p> <p>Here are the h2o groupby query runtimes on a 100 million row (1e8) dataset with various engines:</p> <p></p> <p>Here are the h2o groupby query runtimes on the 1e8 table that run slower:</p> <p></p> <p>Here are the h2o join query runtimes for the 1e8 table that run slower:</p> <p></p>"},{"location":"#clickbench-results","title":"ClickBench results","text":"<p>Here are the ClickBench results with the QueryBench methodology:</p> <p></p> <p></p> <p></p>"},{"location":"#tpc-h-benchmark-results","title":"TPC H benchmark results","text":"<p>Here are the TPC H query results for various scale factors.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"#how-to-analyze-the-querybench-results","title":"How to analyze the QueryBench results","text":"<p>You can see that the benchmarks provide mixed results.</p> <p>You need to dig into the details a bit when you're selecting the best engine for your workflows.</p> <p>Don't rely on overly simplistic studies that reduce benchmarks to a single graph - that's usually too simplistic for an intricate decision.</p>"},{"location":"#querybench-project-goals","title":"QueryBench project goals","text":"<p>QueryBench is helpful for different types of users:</p> <ul> <li>Help practitioners select fast engines for their workflows</li> <li>Allow query engine maintainers to identify areas of improvement</li> <li>Provide benchmarking best practices for the data industry</li> </ul>"},{"location":"#why-benchmarks-are-important","title":"Why benchmarks are important","text":"<p>Benchmarks are essential for a few reasons:</p> <ul> <li>You are more productive when your queries run faster</li> <li>You save money when your queries run faster on virtual machines</li> <li>Efficient engines can handle computationally intense queries that cause inefficient engines to error out</li> </ul> <p>Analyzing modern query engines and selecting the right tool for the job requires considerable time and effort.  It's easier to determine the best options based on reliable benchmarks, determine the engine with functionality that matches your workflows, and start by trying out the best-performing engine.</p> <p>Suppose you'd like to find the quickest way to join a 2 GB CSV file with a 1 GB Parquet file on your local machine.</p> <p>You may not want to perform an exhaustive analysis yourself.  You'll find it easier to look up some benchmarks and make an informed decision on the best alternative.</p> <p>Trying out 10 different options that require figuring out how to use various programming languages isn't realistic.  Benchmarks serve to guide users to informed choices for their use cases, taking into account their time constraints.</p> <p>Benchmarks can be harmful when they're biased or improperly structured, yielding misleading conclusions.  Benchmarks should not intentionally or unintentionally mislead readers into making suboptimal technology choices.</p> <p>Benchmarks should also pave the way for revolutionary technologies to gain adoption.  When a new query engine determines how to process data more efficiently and reliably, it should be able to quantify these improvements to users through benchmarks.  This helps drive adoption.</p>"},{"location":"#the-best-query-engine-depends-on-many-factors","title":"The best query engine depends on many factors","text":"<p>Choosing the best engine is multifaceted, so the performance of the engine for specific queries isn't the only decision factor.</p> <p>Here are other essential factors to consider:</p> <ul> <li>OLTP vs. OLAP query patterns</li> <li>ad hoc vs. consistent querying</li> <li>skill set and preferences of your team</li> </ul> <p>QueryBench aims to provide a variety of benchmarks, enabling you to identify the fastest engine for your team.  It strives to highlight potential biases associated with different query patterns rather than hiding these essential details.</p>"},{"location":"#query-engine-speed-isnt-the-only-factor","title":"Query engine speed isn't the only factor","text":"<p>Please make sure that you put these results into perspective and remember that query engine speed is not the only important factor when selecting a query engine.</p> <p>Here are some other essential factors:</p> <ul> <li>library ecosystem: you may want to use a specific engine that's compatible with a vital library you need for an analysis</li> <li>team skill set: your team may only know Python, so a Java engine isn't a good option for them</li> <li>team preferences: your team may be passionate about the tidyverse and R programming language, so it may not be worth switching to a different ecosystem</li> <li>data set size: your data may be large, so you have to use a big data engine.  Alternatively, if your data is small, there is no need to worry about running a distributed engine.</li> </ul> <p>If you follow the Lakehouse architecture, you can use many engines.</p>"},{"location":"#you-can-use-many-engines","title":"You can use many engines","text":"<p>You don't need to use only a single engine.</p> <p>You can use one engine to run part of a data pipeline and then pass off the analysis to another engine to get the best of both worlds.</p>"},{"location":"#why-the-data-industry-is-skeptical-of-benchmarks","title":"Why the data industry is skeptical of benchmarks","text":"<p>The data community is tired of VendorX publishing biased benchmarks that show that their QueryEngineX is the best for all workflows.</p> <p>You've probably seen the common errors:</p> <ul> <li>a study that tunes the queries for QueryEngineX and doesn't tune the other engines</li> <li>carefully selected queries that work exceptionally well for a specific engine</li> <li>hardware that works best for their query engine</li> <li>using engines far from their designed purpose, like using an engine that's built to be distributed amongst many nodes in a cluster on a single node</li> <li>using a file format such that filesystem I/O takes more time than actually executing the query</li> </ul>"},{"location":"datasets/","title":"Datasets","text":""},{"location":"datasets/#h20","title":"h20","text":""},{"location":"datasets/#h2o-groupby-table","title":"h2o groupby table","text":"<p>/Users/matthewpowers/data/G1_1e8_1e2_0_0.parquet shape: (100_000_000, 9)</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id1   \u2506 id2   \u2506 id3          \u2506 id4 \u2506 \u2026 \u2506 id6    \u2506 v1  \u2506 v2  \u2506 v3        \u2502\n\u2502 ---   \u2506 ---   \u2506 ---          \u2506 --- \u2506   \u2506 ---    \u2506 --- \u2506 --- \u2506 ---       \u2502\n\u2502 str   \u2506 str   \u2506 str          \u2506 i64 \u2506   \u2506 i64    \u2506 i64 \u2506 i64 \u2506 f64       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 id016 \u2506 id046 \u2506 id0000109363 \u2506 88  \u2506 \u2026 \u2506 146094 \u2506 4   \u2506 6   \u2506 18.837686 \u2502\n\u2502 id039 \u2506 id087 \u2506 id0000466766 \u2506 14  \u2506 \u2026 \u2506 111330 \u2506 4   \u2506 14  \u2506 46.797328 \u2502\n\u2502 id047 \u2506 id098 \u2506 id0000307804 \u2506 85  \u2506 \u2026 \u2506 187639 \u2506 3   \u2506 5   \u2506 47.577311 \u2502\n\u2502 id043 \u2506 id017 \u2506 id0000344864 \u2506 87  \u2506 \u2026 \u2506 256509 \u2506 2   \u2506 5   \u2506 80.462924 \u2502\n\u2502 id054 \u2506 id027 \u2506 id0000433679 \u2506 99  \u2506 \u2026 \u2506 32736  \u2506 1   \u2506 7   \u2506 15.796662 \u2502\n\u2502 \u2026     \u2506 \u2026     \u2506 \u2026            \u2506 \u2026   \u2506 \u2026 \u2506 \u2026      \u2506 \u2026   \u2506 \u2026   \u2506 \u2026         \u2502\n\u2502 id080 \u2506 id025 \u2506 id0000598386 \u2506 43  \u2506 \u2026 \u2506 56728  \u2506 3   \u2506 9   \u2506 27.47907  \u2502\n\u2502 id064 \u2506 id012 \u2506 id0000844471 \u2506 19  \u2506 \u2026 \u2506 203895 \u2506 4   \u2506 5   \u2506 5.323666  \u2502\n\u2502 id046 \u2506 id053 \u2506 id0000544024 \u2506 31  \u2506 \u2026 \u2506 711000 \u2506 5   \u2506 3   \u2506 27.827385 \u2502\n\u2502 id081 \u2506 id090 \u2506 id0000802094 \u2506 53  \u2506 \u2026 \u2506 57466  \u2506 1   \u2506 15  \u2506 23.319917 \u2502\n\u2502 id001 \u2506 id057 \u2506 id0000141978 \u2506 93  \u2506 \u2026 \u2506 224681 \u2506 4   \u2506 10  \u2506 91.788497 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Schema({'id1': String, 'id2': String, 'id3': String, 'id4': Int64, 'id5': Int64, 'id6': Int64, 'v1': Int64, 'v2': Int64, 'v3': Float64})</p> <p>/Users/matthewpowers/data/J1_1e8_NA_0_0.parquet shape: (100_000_000, 7)</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id1 \u2506 id2    \u2506 id3       \u2506 id4   \u2506 id5      \u2506 id6         \u2506 v1        \u2502\n\u2502 --- \u2506 ---    \u2506 ---       \u2506 ---   \u2506 ---      \u2506 ---         \u2506 ---       \u2502\n\u2502 i64 \u2506 i64    \u2506 i64       \u2506 str   \u2506 str      \u2506 str         \u2506 f64       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 32  \u2506 57316  \u2506 104012378 \u2506 id32  \u2506 id57316  \u2506 id104012378 \u2506 2.184703  \u2502\n\u2502 17  \u2506 32099  \u2506 103369135 \u2506 id17  \u2506 id32099  \u2506 id103369135 \u2506 26.295686 \u2502\n\u2502 106 \u2506 102270 \u2506 66344514  \u2506 id106 \u2506 id102270 \u2506 id66344514  \u2506 34.744782 \u2502\n\u2502 99  \u2506 51861  \u2506 79312153  \u2506 id99  \u2506 id51861  \u2506 id79312153  \u2506 73.818861 \u2502\n\u2502 11  \u2506 28655  \u2506 51482959  \u2506 id11  \u2506 id28655  \u2506 id51482959  \u2506 66.362821 \u2502\n\u2502 \u2026   \u2506 \u2026      \u2506 \u2026         \u2506 \u2026     \u2506 \u2026        \u2506 \u2026           \u2506 \u2026         \u2502\n\u2502 13  \u2506 22767  \u2506 35069816  \u2506 id13  \u2506 id22767  \u2506 id35069816  \u2506 94.651984 \u2502\n\u2502 72  \u2506 99663  \u2506 44320313  \u2506 id72  \u2506 id99663  \u2506 id44320313  \u2506 32.654356 \u2502\n\u2502 110 \u2506 4985   \u2506 22435441  \u2506 id110 \u2506 id4985   \u2506 id22435441  \u2506 75.312469 \u2502\n\u2502 17  \u2506 4136   \u2506 85575483  \u2506 id17  \u2506 id4136   \u2506 id85575483  \u2506 63.577894 \u2502\n\u2502 70  \u2506 75769  \u2506 19286096  \u2506 id70  \u2506 id75769  \u2506 id19286096  \u2506 49.151411 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Schema({'id1': Int64, 'id2': Int64, 'id3': Int64, 'id4': String, 'id5': String, 'id6': String, 'v1': Float64})</p> <p>/Users/matthewpowers/data/J1_1e8_1e2_0_0.parquet shape: (100, 3)</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id1 \u2506 id4   \u2506 v2        \u2502\n\u2502 --- \u2506 ---   \u2506 ---       \u2502\n\u2502 i64 \u2506 str   \u2506 f64       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 19  \u2506 id19  \u2506 53.89299  \u2502\n\u2502 96  \u2506 id96  \u2506 35.865322 \u2502\n\u2502 44  \u2506 id44  \u2506 66.587577 \u2502\n\u2502 91  \u2506 id91  \u2506 12.940303 \u2502\n\u2502 31  \u2506 id31  \u2506 2.883551  \u2502\n\u2502 \u2026   \u2506 \u2026     \u2506 \u2026         \u2502\n\u2502 69  \u2506 id69  \u2506 32.144187 \u2502\n\u2502 82  \u2506 id82  \u2506 43.766849 \u2502\n\u2502 66  \u2506 id66  \u2506 43.799275 \u2502\n\u2502 105 \u2506 id105 \u2506 94.711328 \u2502\n\u2502 81  \u2506 id81  \u2506 69.904453 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Schema({'id1': Int64, 'id4': String, 'v2': Float64})</p> <p>/Users/matthewpowers/data/J1_1e8_1e5_0_0.parquet shape: (100_000, 5)</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id1 \u2506 id2    \u2506 id4   \u2506 id5      \u2506 v2        \u2502\n\u2502 --- \u2506 ---    \u2506 ---   \u2506 ---      \u2506 ---       \u2502\n\u2502 i64 \u2506 i64    \u2506 str   \u2506 str      \u2506 f64       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 69  \u2506 82839  \u2506 id69  \u2506 id82839  \u2506 79.322039 \u2502\n\u2502 94  \u2506 65192  \u2506 id94  \u2506 id65192  \u2506 26.282094 \u2502\n\u2502 27  \u2506 103858 \u2506 id27  \u2506 id103858 \u2506 51.550879 \u2502\n\u2502 10  \u2506 40683  \u2506 id10  \u2506 id40683  \u2506 84.647495 \u2502\n\u2502 42  \u2506 5979   \u2506 id42  \u2506 id5979   \u2506 83.488062 \u2502\n\u2502 \u2026   \u2506 \u2026      \u2506 \u2026     \u2506 \u2026        \u2506 \u2026         \u2502\n\u2502 42  \u2506 95337  \u2506 id42  \u2506 id95337  \u2506 32.217377 \u2502\n\u2502 104 \u2506 55177  \u2506 id104 \u2506 id55177  \u2506 39.670606 \u2502\n\u2502 14  \u2506 46220  \u2506 id14  \u2506 id46220  \u2506 55.6271   \u2502\n\u2502 31  \u2506 79430  \u2506 id31  \u2506 id79430  \u2506 52.355275 \u2502\n\u2502 60  \u2506 10612  \u2506 id60  \u2506 id10612  \u2506 64.503299 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Schema({'id1': Int64, 'id2': Int64, 'id4': String, 'id5': String, 'v2': Float64})</p> <p>/Users/matthewpowers/data/J1_1e8_1e8_0_0.parquet shape: (100_000_000, 7)</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id1 \u2506 id2   \u2506 id3      \u2506 id4   \u2506 id5     \u2506 id6        \u2506 v2        \u2502\n\u2502 --- \u2506 ---   \u2506 ---      \u2506 ---   \u2506 ---     \u2506 ---        \u2506 ---       \u2502\n\u2502 i64 \u2506 i64   \u2506 i64      \u2506 str   \u2506 str     \u2506 str        \u2506 f64       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 107 \u2506 53407 \u2506 81930178 \u2506 id107 \u2506 id53407 \u2506 id81930178 \u2506 75.634881 \u2502\n\u2502 30  \u2506 44458 \u2506 73257490 \u2506 id30  \u2506 id44458 \u2506 id73257490 \u2506 53.043222 \u2502\n\u2502 104 \u2506 89910 \u2506 38361265 \u2506 id104 \u2506 id89910 \u2506 id38361265 \u2506 83.067211 \u2502\n\u2502 14  \u2506 74193 \u2506 47636586 \u2506 id14  \u2506 id74193 \u2506 id47636586 \u2506 59.066146 \u2502\n\u2502 49  \u2506 77202 \u2506 95213755 \u2506 id49  \u2506 id77202 \u2506 id95213755 \u2506 60.308764 \u2502\n\u2502 \u2026   \u2506 \u2026     \u2506 \u2026        \u2506 \u2026     \u2506 \u2026       \u2506 \u2026          \u2506 \u2026         \u2502\n\u2502 30  \u2506 26807 \u2506 87061377 \u2506 id30  \u2506 id26807 \u2506 id87061377 \u2506 32.629123 \u2502\n\u2502 14  \u2506 73040 \u2506 40089145 \u2506 id14  \u2506 id73040 \u2506 id40089145 \u2506 54.944554 \u2502\n\u2502 100 \u2506 49006 \u2506 98750911 \u2506 id100 \u2506 id49006 \u2506 id98750911 \u2506 11.757998 \u2502\n\u2502 17  \u2506 89387 \u2506 16056323 \u2506 id17  \u2506 id89387 \u2506 id16056323 \u2506 31.216292 \u2502\n\u2502 53  \u2506 67188 \u2506 12279067 \u2506 id53  \u2506 id67188 \u2506 id12279067 \u2506 0.275111  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Schema({'id1': Int64, 'id2': Int64, 'id3': Int64, 'id4': String, 'id5': String, 'id6': String, 'v2': Float64}) ```</p>"},{"location":"datasets/#h2o-join-tables","title":"h2o join tables","text":"<p>The h2o join tables are ...</p>"},{"location":"datasets/#clickbench","title":"ClickBench","text":"<p>TODO</p>"},{"location":"datasets/#polars-decision-support-pds-h","title":"Polars Decision Support PDS-H","text":"<p>TODO</p>"},{"location":"good-benchmarks/","title":"How to write good data engine benchmarks","text":"<p>This page shows how to write high-quality data engine benchmarks.</p> <p>The data industry benefits from better benchmarks.</p>"},{"location":"good-benchmarks/#properties-of-good-benchmarks","title":"Properties of good benchmarks","text":"<p>accessible dataset</p> <p>Users should be able to easily download or generate datasets to replicate the queries on their machine.</p> <p>list all software verions</p> <p>Make sure to list software versions for all the engines you use in addition to any other relevant information.</p> <p>For exmaple, you should specify that a given benchmark uses Polars v1.31.0 and uses the Polars streaming engine.</p> <p>TODO</p> <p>list all hardware specs</p> <p>TODO</p> <p>open source benchmarking code</p> <p>TODO</p> <p>don't use any methdologies that clearly favor one engine without disclosing</p> <p>TODO</p>"},{"location":"good-benchmarks/#benchmarking-is-hard","title":"Benchmarking is hard","text":"<p>It's difficult to build accurate benchmarks.  Runtimes depends on the hardware, software versions, and data setup.</p> <p>Accurate benchmarks are even harder when comparing different technologies.  Certain frameworks will perform better with different files sizes and file formats.  This benchmarking analysis tries to give a fair representation on the range of outcomes that are possible given the most impactful inputs.</p> <p>The benchmarks presented in this repo should not be interpreted as definitive results.  They're runtimes for specific data tasks, on one type of hardware, with a specific set of software versions.  The code isn't necessarily optimized (we accept community contributions to restructure code).</p> <p>The data community should find these benchmarks valuable, caveats aside.</p>"},{"location":"methodology/","title":"QueryBench Methodology","text":"<p>QueryBench offers various benchmarks to account for trade-offs with different engines.</p> <p>Here are some guiding principles:</p> <ol> <li>Show the good and bad sides of each engine</li> <li>Use entirely different query types to show edge case performance (e.g., show group by aggregations on a single low cardinality column and many high cardinality columns)</li> <li>Build benchmarks that are easy to reproduce</li> <li>Benchmark total runtime, including the time it takes to load into memory</li> <li>Don't allow a single query to bias the overall results</li> </ol> <p>QueryBench uses a benchmarking methodology that's fair for all engines.</p> <p>The queries are designed to highlight the strengths and weaknesses of engines.</p> <p>The methodology is designed to level out obvious biases found in other benchmarks.  Let's begin by examining some of the queries.</p>"},{"location":"methodology/#query-selection","title":"Query selection","text":"<p>It's good to stress test a query engine with different types of queries:</p> <ul> <li>aggregations on low and high cardinality columns</li> <li>aggregations with one group by column and many group by columns</li> <li>joins with a large table + a small table and two large tables</li> <li>sorting operations on low and high cardinality columns</li> <li>compound queries with many operations like a join, then a filter, then a sort</li> </ul> <p>You should select a representative basket of queries.  This is a great way to expose opportunities for optimizations in engines:</p> <ul> <li>One engine may need to get better when running group-by queries on lots of columns</li> <li>Another engine may struggle when joining two large engines</li> <li>A third engine may need a better optimizer for compound queries</li> </ul> <p>The DataFusion community has successfully utilized this iterative process to enhance the speed of their engine further.</p>"},{"location":"methodology/#in-memory-queries-vs-cold-queries","title":"In-memory queries vs. cold queries","text":"<p>There are two different ways to run queries:</p> <ul> <li>in-memory: load all the data into memory and then quantify the runtime after the data is already loaded</li> <li>cold queries: the query runtime to load data from disk and execute the query</li> </ul> <p>Either methodology can be a better representation of your query pattern, depending on how you analyze data.  If you have data loaded in a data warehouse and run queries, then in-memory queries are more representative.  If you store data in cloud storage and run ad hoc queries, then cold queries are more representative.</p> <p>It's best to present the runtimes for both types of queries to give the most accurate representation of runtimes.</p>"},{"location":"methodology/#dataset-sizes","title":"Dataset sizes","text":"<p>It's beneficial to display query runtimes for various scale factors to illustrate how engines perform under different loads.  Here are good dataset sizes:</p> <ul> <li>tiny tables</li> <li>medium-sized tables that easily fit into memory</li> <li>large tables that can't fit into memory and challenge the engine</li> <li>larger than memory datasets to see if an engine can handle out-of-memory engines</li> </ul> <p>Distributed engines should be benchmarked with large dataset sizes that can't fit on a single machine.</p> <p>Some other benchmarking studies only use small-scale factors on a single machine for distributed machines that are meant to be run on huge tables, which is misleading.</p> <p>Ensure you use table sizes that align with your hardware specifications and query engine.</p>"},{"location":"methodology/#file-formats","title":"File formats","text":"<p>Benchmarks can be built with different file formats:</p> <ul> <li>Row-based formats like CSV, JSON, or Avro</li> <li>Columnar file formats like Parquet and ORC</li> </ul> <p>Some benchmarks use row-based formats, which can force columns unrelated to the query to be needlessly loaded into memory.  These benchmarks can end up spending more time testing an engine's CSV reader than the query execution capabilities.</p> <p>It's generally better to use columnar file formats, such as Parquet, to more accurately measure an engine's query execution capabilities.</p>"},{"location":"methodology/#lazy-vs-eager-computations","title":"Lazy vs. eager computations","text":"<p>Some engines execute queries eagerly, whereas others execute computations lazily, deferring them until the last possible moment when the user needs the results.</p> <p>Pandas is an eager engine, and DataFusion is a lazy engine, for example.</p> <p>Please ensure that you fully execute queries with every engine for a fair comparison.</p>"},{"location":"methodology/#single-node-vs-distributed-compute-engines","title":"Single node vs. distributed compute engines","text":"<p>Some engines are designed to run on a single machine, while others are distributed engines intended to run on a cluster.</p> <p>Comparing single-machine engines and distributed engines is somewhat pointless:</p> <ul> <li>Distributed engines have lots of overhead and will run slower than single-machine engines on a small dataset</li> <li>Single-machine engines will error out for massive datasets that require a cluster</li> </ul> <p>It may be interesting to run some distributed engine queries on a single machine if you're curious about how your Apache Spark test suite performs locally.</p> <p>But don't make the mistake of extrapolating Apache Spark vs. pandas benchmarks on a small dataset on a single machine to other dataset sizes.</p>"},{"location":"methodology/#using-the-same-methodology-for-all-engines","title":"Using the same methodology for all engines","text":"<p>Query engines/databases/data warehouses are used and architected quite differently, so they need different benchmarking methodologies.</p> <p>If your data warehouse always has data loaded in memory, then you can benchmark the hot queries.</p> <p>If you only run ad hoc queries for data in cloud storage, then cold run queries are the most useful.</p> <p>QueryBench aims to display the methodology that's most reliable for the different types of query engines.</p>"},{"location":"methodology/#olap-vs-oltp-engines","title":"OLAP vs. OLTP engines","text":"<p>An engine that's optimized for OLAP queries typically cannot perform OLTP queries as quickly.</p> <p>A set of queries that shows an OLAP engine can perform OLAP queries faster than an OLTP engine isn't exciting.</p> <p>It's better to demonstrate how one engine can perform OLAP queries faster, while the other excels at OLTP workflows.</p>"},{"location":"queries/","title":"Queries","text":"<p>This page covers the logic behind the various queries in QueryBench.</p>"},{"location":"queries/#querybench","title":"QueryBench","text":""},{"location":"queries/#single-table","title":"Single table","text":"<p>TODO: Add some single table queries on the Clickhouse table</p>"},{"location":"queries/#h2o","title":"h2o","text":""},{"location":"queries/#h2o-join","title":"h2o join","text":"<p>Note: This section assums the 1e8 dataset size</p> <p>The first h2o join query runs a join operation on a large table and a small table:</p> <pre><code>SELECT x.id1, x.id2, x.id3, x.id4 as xid4, small.id4 as smallid4, x.id5, x.id6, x.v1, small.v2\nFROM x\nINNER JOIN small ON x.id1 = small.id1\n</code></pre> <p>This query returns XX rows.</p>"},{"location":"queries/#h2o-groupby","title":"h2o groupby","text":"<p>Note: This section assums the 1e8 dataset size</p> <p>The first h2o groupby query runs a group by aggregation on a low cardinality column:</p> <pre><code>select id1, sum(v1) as v1\nfrom x\ngroup by id1\n</code></pre> <p>The <code>id1</code> column has 100 distinct values, so this query returns 100 results.  Here's what the result looks like:</p> <p>XXX</p> <p>The second h2o group by query runs a group by aggregation on two columns:</p> <pre><code>select id1, id2, sum(v1) as v1 \nfrom x\ngroup by id1, id2\n</code></pre> <p>Each id1 value has 100 distinct id2 values, so this query returns 10,000 results.  Here's what they look like:</p> <p>XXX</p> <p>TODO</p>"},{"location":"queries/#clickbench","title":"Clickbench","text":"<p>TODO</p>"},{"location":"other-benchmarks/clickbench-pros-cons/","title":"ClickBench Benchmarks Pros and Cons","text":"<p>The Clickbench benchmarks are based on a dataset with 99,537,185 rows and XX columns.  The <code>hits.parquet</code> file is 14.8 GB in storage.</p> <p>They contain queries like the following:</p> <ul> <li>Select an average from a column: <code>SELECT AVG(\"UserID\") FROM hits;</code></li> <li>Aggregation then sort: <code>SELECT \"UserID\", COUNT(*) FROM hits GROUP BY \"UserID\" ORDER BY COUNT(*) DESC LIMIT 10;</code></li> <li>Pattern matching: <code>SELECT * FROM hits WHERE \"URL\" LIKE \\'%google%\\' ORDER BY \"EventTime\" LIMIT 10;</code></li> </ul> <p>The benchmarks are displayed as a cumulative runtime of all the queries.</p> <p>Here's an example of the comparison they present for DataFusion vs. Daft:</p> <p>XXX</p> <p>And here's the comparison for a select few queries:</p> <p>XXX</p> <p>Let's turn our attention to the wonderful benefits of the Clickbench benchmarks.</p>"},{"location":"other-benchmarks/clickbench-pros-cons/#benefits-of-the-clickbench-benchmarks","title":"Benefits of the ClickBench benchmarks","text":"<p>The Clickbench benchmarks are easy to understand, cover a variety of different query types, and are useful for end users and query engine maintainers.  They make it relatively easy to spot queries where a given engine underperforms.</p> <p>Covers lots of different engines</p> <p>The Clickbench benchmarks cover many engines, including Daft, Polars, and TODO (add more).</p> <p>Some engines are missing, notably TODO.</p> <p>Good variety of queries</p> <p>XXX</p> <p>Easily accessible dataset</p> <p>The Clickbench dataset can be downloaded in the project README.</p> <p>This is a significant improvement compared to the h2o dataset, which requires running an R program, which can be challenging if you aren't familiar with the R programming language.</p> <p>Data in Parquet files</p> <p>The Clickbench dataset is distributed in different file formats, including Parquet, which is ideal for benchmarks. Parquet is columnar, allowing for column pruning, has a well-defined schema, so engines don't need to infer column names or types, and is binary, making storage files on disk relatively small.</p> <p>File formats like CSV can cause complications when comparing two engines.</p> <p>Cover hot and cold query times</p> <p>The Clickbench queries include query runtimes when data is already loaded in memory and when data is on disk and not loaded yet.</p> <p>This provides a good representation of runtimes for databases when data is already loaded, as well as for enterprises that run ad hoc queries on data stored on disk.</p> <p>Different machine sizes are used</p> <p>XXX</p>"},{"location":"other-benchmarks/clickbench-pros-cons/#disadvantages-of-the-clickbench-benchmarks","title":"Disadvantages of the Clickbench benchmarks","text":"<p>The Clickbench benchmarks are excellent, but don't test important types of queries and have many flaws.</p> <p>Let's review the limitations of Clickbench so that you can interpret the results.</p> <p>No join queries</p> <p>The Clickbench queries only cover single-table queries and don't consider joins or compound queries with joins.</p> <p>Joins are often the biggest bottleneck for a query, so this is a significant omission.</p> <p>Only one dataset size</p> <p>All the Clickhouse benchmarks are run on the table with XX rows.</p> <p>The benchmarks aren't run on datasets with different scale factors.  It'd be interesting to see the same benchmarks run on the same table with XX rows, XX rows, etc.</p> <p>This could stress test engines that have issues when the load stresses the limits of the hardware for a machine.</p> <p>One slow query has a significant impact on results</p> <p>The query times for all 42 queries are presented in total, which means that a single slow-running query can greatly impact the overall result. Ranking the queries based on their ranking for all queries would be another reliable way to present the results.</p> <p>Not useful for specialized query engines</p> <p>Some of the query engines in Clickbench are specialized for a use case and aren't optimized general-purpose engines:</p> <ul> <li>Time is for time series analyses</li> <li>XX is for OLTP</li> <li>XX is for XX</li> </ul> <p>It's better to benchmark specialized query engines with specialized query patterns.  For example, a spatial query engine should be benchmarked using spatial queries and compared with other spatial engines.</p> <p>Bad for distributed query engines</p> <p>Properly testing a distributed engine requires large datasets spread across many files and a computation cluster with several machines.</p> <p>The Clickbench dataset is small, and the queries can be run on a single laptop with only 16 GB of memory.  It's way too small of a dataset to test a distributed engine.</p> <p>The Clickbench dataset could be 10 times larger and that would give an interesting 147.8 GB dataset to test a distributed engine.</p> <p>Some metadata-only queries</p> <p>The first query can be computed from the Parquet metadata alone: <code>\"SELECT COUNT(*) FROM hits;\"</code>.</p> <p>The query engine can fetch the row count from the metadata of every row group in the file and compute the sum.</p> <p>It's essential to verify that engines utilize metadata to run queries faster when possible, but it's not particularly interesting after that has been confirmed.</p> <p>Lots of quick queries</p> <p>A large portion of the queries (around 26) can be run in under a second on a MacBook M3 with 16 GB of RAM.  </p> <p>Ten queries finish between 1 and 3 seconds.</p> <p>The remaining six queries take more than 3 seconds to run.</p> <p>The queries are skewed to lightweight workflows that can run very quickly.</p>"},{"location":"other-benchmarks/clickbench-pros-cons/#conclusion","title":"Conclusion","text":"<p>Querybench is an excellent benchmarking analysis and a significant contribution to the data community.</p> <p>It's fantastic that Clickhouse has open-sourced the dataset and the code.  It's also amazing that they maintain the benchmarks and made a nice user interface.</p>"},{"location":"other-benchmarks/h2o-pros-cons/","title":"h2o benchmarks pros and cons","text":"<p>The h2o benchmarks have certain limitations, as do all benchmarks.</p> <p>This section explains some of the limitations of the h2o benchmarks, not as a criticism, but to explain the tradeoffs that were made in the h2o benchmarks.  The h2o benchmarks are an excellent contribution to the data community and we should be grateful for the engineers that dedicated their time and effort to make them available.</p>"},{"location":"other-benchmarks/h2o-pros-cons/#single-csv-file","title":"Single CSV file","text":"<p>The h2o benchmarks are run on data that's stored in a single CSV file.</p> <p>This introduces a bias for query engines that are optimzied for running on a single file.  This bias is somewhat offset because the data is persisted in memory before the queries are run.</p> <p>Remember that CSV is a row based file format and column projection is not supported.  You can't cherry pick certain columns and persist them in memory when running queries.</p> <p>CSVs also require for schemas to be inferred or manually specified.  If one engine manually specifies an efficient type (e.g. int32) and another engine infers a less efficient type (e.g. int64) for a given column, then the query comparisons are biased.  Having all engines use the same types defined in the Parquet file provides a more even comparison.</p>"},{"location":"other-benchmarks/h2o-pros-cons/#data-persisted-in-memory","title":"Data persisted in memory","text":"<p>The h2o benchmarks persist data in memory, but they are using CSV files, so they need to persist all the data in memory at once.  They can't just persist the columns that are relevant to the query.  Persisting all the columns causes certain queries to error out that wouldn't otherwise have issues if only 2 or 3 columns were persisted.</p> <p>Persisting data in memory also hides performance benefits of querie engines that are capable of performing parallel I/O.</p>"},{"location":"other-benchmarks/h2o-pros-cons/#presenting-overall-query-runtime-is-misleading","title":"Presenting overall query runtime is misleading","text":"<p>The h2o benchmarks show the runtimes for each individual query and all the queries summed.  The sum amount can be greatly impacted by one slowly running query, so it's potentially misleading.</p>"},{"location":"other-benchmarks/h2o-pros-cons/#engines-that-support-parallel-io","title":"Engines that support parallel I/O","text":"<p>Certain query engines are designed to read and write data in parallel - others are not.</p> <p>Query engine users often care about the entire time it takes to run a query.  How long it takes to read the data, persist it in memory, and execute the query.  The h2o benchmarks only give readers information on how long it takes to actually execute the query.  This is certainly valuable information, but potentially misleading for the 50 GB benchmarks.  Reading 50 GB of data is a significant performance advantage compared to reading a single file.</p>"},{"location":"other-benchmarks/h2o-pros-cons/#small-datasets","title":"Small datasets","text":"<p>The h2o benchmarks only test 0.5 GB, 5 GB, and 50 GB datasets.  They don't test 500 GB or 5 TB datasets.</p> <p>This introduces a bias for query engines that can run on small datasets, but can't work on larger datsets.</p>"},{"location":"other-benchmarks/h2o-pros-cons/#distributed-engines-vs-single-machine-engines","title":"Distributed engines vs single machine engines","text":"<p>Distributed query engines usually make tradeoffs so they're optimized to be run on a cluster of computers.  The h2o benchmarks are run on a single machine, so they're biased against distributed query engines.</p> <p>Distributed query engines of course offer a massive benefit for data practitioners.  They can be scaled beyond the limits of a single machine and can be used to run analysis on large datasets.  Including benchmarks on larger datasets with distributed clusters is a good way to highlight the strenghts of query engines that are designed to scale.  This is also a good way to highlight the data volume limits of query engines designed to run on a single machine.</p>"},{"location":"other-benchmarks/h2o-pros-cons/#gpus-vs-cpus","title":"GPUs vs CPUs","text":"<p>The h2o bencharks include query engines that are designed to be run on CPUs and others that are designed to be run on GPUs.  There are a variety of ways to present benchmarking results from different hardware:</p> <ul> <li>present the results side-by-side, in the same graph</li> <li>present the results in different graphs</li> <li>present cost adjusted results</li> </ul> <p>h2o decided to present the results side-by-side, which is reasonable, but there are other ways these results could habe been presented too.</p>"},{"location":"other-benchmarks/h2o-pros-cons/#query-optimization","title":"Query optimization","text":"<p>The h2o benchmarks are well specified, so they don't give engines that have query optimizers a chance to shine.  Engines with a query optimizer will rearrange a poorly specified query and make it run better.  The h2o benchmarks could have included some poorly specified queries to highlight they query optimization strenghts of certain engines.</p>"},{"location":"other-benchmarks/h2o-pros-cons/#lazy-computations-and-collecting-results","title":"Lazy computations and collecting results","text":"<p>Certain query engines support lazy execution, so they don't actually run computations until it's absolutely necessary.  Other query engines eagerly execute computations whenever a command is run.</p> <p>Lazy computation engines generally split up data and execute computations in parallel.  They don't collect results into a single DataFrame by default because it's usually better to keep the data split up for additional parallelism for downstream computations.</p> <p>Running a query for a lazy computation engine generally involves two steps:</p> <ul> <li>actually running the computation</li> <li>collecting the subresults into a single DataFrame</li> </ul> <p>Collecting the results into a single DataFrame arguably should not be included in the parallel engine computation runtime.  That's an extra step that's required to get the result, but not usually necessary in a real-world use case.</p> <p>It can unfortunately be hard to divide a query runtime into different compontents.  Most parallel compute engine query runtimes include both, which is probably misleading.</p>"},{"location":"other-benchmarks/polars-decision-support-pds-h/","title":"Pros and cons of the Polars Decision Support (PDS-H) Benchmarks","text":"<p>The PDS-H benchmarks are an adaptation of the TPC-H benchmarks.</p>"}]}